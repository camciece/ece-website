---
title:
  en: How LLMs work?
  tr: 'Ä°nsansÄ± hissettiren bir matematik sistemi: LLMâ€™ler nasÄ±l Ã§alÄ±ÅŸÄ±yor?'
image: /LLMs.png
shareDescription: true
excerpt:
  en:
  tr: 'LLM Mimarisi 101: Transformer, Embeddings ve Attention mekanizmasÄ±yla yapay zekanÄ±n insansÄ± cevaplar Ã¼retme sÃ¼reci'
date: 27 Ocak 2026
tags:
  - AI
  - Ops
content:
  en: |
    Fast prototypes are fun. But real systems survive because of the dull bits:
    runbooks, quotas, monitoring, budgets, and incident response.

    You can spend more than you think, and you wonâ€™t see it until a month later.
    â€œBoringâ€ controls make the cost visible.

  tr: |

    BugÃ¼n yapay zeka dendiÄŸinde Claude, Gemini ve GPT gibi dev isimlerin rekabetini konuÅŸuyoruz. Claude, Ã¶zellikle uzun dokÃ¼man analizi ve kod yazÄ±mÄ± gibi "disiplin" gerektiren iÅŸ akÄ±ÅŸlarÄ±nda gÃ¼venilir bir liman olarak gÃ¶rÃ¼lÃ¼yor. Gemini, Googleâ€™Ä±n arama ve medya ekosisteminden gelen kas gÃ¼cÃ¼yle, metinden videoya kadar her ÅŸeyi aynÄ± anda anlamlandÄ±ran (multimodal) tarafta iddialÄ± bir Ã§izgi Ã§ekiyor. OpenAI GPT serisiyle, genel muhakeme yeteneÄŸi en dengeli ve geniÅŸ yetenek setine sahip modelleri konumlandÄ±rmaya devam ediyor.

    Peki, bu modellerin cevap stillerini, "karakterlerini", gÃ¼venlik katmanlarÄ±nÄ±, vb. bu kadar farklÄ± kÄ±lan ne? Ä°ÅŸin mutfaÄŸÄ±na girdiÄŸimizde; **Tokenizer** seÃ§imlerinden eÄŸitim verisindeki (pre-training) aÄŸÄ±rlÄ±klara, **RLHF** (Ä°nsan Geri Bildirimiyle PekiÅŸtirmeli Ã–ÄŸrenme) sÃ¼reÃ§lerinden gÃ¼venlik filtrelerine kadar pek Ã§ok deÄŸiÅŸkenin bu farklarÄ± yarattÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼yoruz.

    Ancak tÃ¼m bu farklÄ± kimliklerin arkasÄ±nda, kalbi aynÄ± fikirle atan bir makine var: Hepsi **Transformer** tabanlÄ± ve Ã¶zÃ¼nde **"next token prediction"** (bir sonraki parÃ§ayÄ± tahmin etme) ilkesiyle Ã§alÄ±ÅŸÄ±yorlar. Yani hepsi, Ã¶nÃ¼ne koyduÄŸunuz metne bakÄ±p *"Buradan sonra gelmesi en muhtemel parÃ§a nedir?"* sorusunu milyarlarca kez soran devasa birer olasÄ±lÄ±k motoru.

       <div className="writingArticle__callout">
         Bu yazÄ±da, bu motorun iÃ§ini parÃ§a parÃ§a aÃ§acaÄŸÄ±z:
         <ul className="writingArticle__ArchitectureList">
           <li>Metnin model iÃ§inde nasÄ±l sayÄ±lara (vektÃ¶rlere) dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼,</li>
           <li>Bu sayÄ±larÄ±n hangi matematiksel duraklardan geÃ§erek "anlam" kazandÄ±ÄŸÄ±nÄ±,</li>
           <li>Modelin baÄŸlamÄ± nasÄ±l tarttÄ±ÄŸÄ±nÄ± ve neden her cevabÄ±n kelime kelime Ã¼retildiÄŸini gÃ¶receÄŸiz.</li>
         </ul>
          </div>

        <h2>LLM Mimarisi</h2>

      Peki, bize bu kadar "insansÄ±" cevaplar veren o muazzam matematik sistemi iÃ§eride nasÄ±l Ã§alÄ±ÅŸÄ±yor? DÄ±ÅŸarÄ±dan bakÄ±nca hiper-Ã¶lÃ§ekli, aÅŸÄ±rÄ± karmaÅŸÄ±k ve biraz da gÃ¶z korkutucu gÃ¶rÃ¼ndÃ¼ÄŸÃ¼ doÄŸru. Ama iÅŸin asÄ±l ilginÃ§ tarafÄ± ÅŸu: BÃ¼yÃ¼k dil modellerinin iÃ§ mimarisi, anlatÄ±ldÄ±ÄŸÄ±nda beklediÄŸinizden Ã§ok daha dÃ¼zenli ve kendi iÃ§inde sÃ¼rekli tekrarlanan bir akÄ±ÅŸa sahip.

      Bu akÄ±ÅŸÄ± anlamanÄ±n en kolay yolu, modeli tek parÃ§a bir "zihin" gibi dÃ¼ÅŸÃ¼nmek yerine, bir Ã¼retim bandÄ± gibi hayal etmek. Metin bir uÃ§tan giriyor, birkaÃ§ temel aÅŸamadan geÃ§iyor ve sonunda tekrar metin olarak diÄŸer uÃ§tan Ã§Ä±kÄ±yor. Biz bu sÃ¼rece (pipeline) dÃ¶rt ana bÃ¶lÃ¼mde bakabiliriz:
            
      <figure className="writingArticle__media">
         <img src="/graph.svg" alt="graph" data-graph="true" />
       </figure>
          <ol className="writingArticle__TokenizerList writingArticle__TokenizerList--ordered">
           <li>**Tokenizer:** Metni kÃ¼Ã§Ã¼k parÃ§alara ayÄ±rÄ±p sayÄ±sal etiketlere dÃ¶nÃ¼ÅŸtÃ¼ren, fabrikamÄ±zÄ±n giriÅŸ kapÄ±sÄ±</li>
           <li>**Embeddings:** SayÄ±sal etiketlerin, kelime anlamlarÄ±nÄ± temsil eden Ã§ok boyutlu bir koordinat sistemine yerleÅŸtirildiÄŸi katman</li>
           <li>**Transformer (Attention & Feedforward):** Kelimelerin birbirine bakÄ±p anlam kazandÄ±ÄŸÄ± ve milyarlarca Ã§arpma iÅŸlemiyle bir sonraki adÄ±mÄ± hesaplayan asÄ±l "tahmin" motoru</li>
           <li>**Output:** Modelin bir sonraki adÄ±m iÃ§in hesapladÄ±ÄŸÄ± sayÄ±sal olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ±nÄ±n Ã¼retildiÄŸi yer</li>
         </ol>

         Burada Ã¶zellikle Transformer katmanÄ±nda dÃ¶nen iÅŸlem hacmi devasa olsa da, mimarinin kendisi ÅŸaÅŸÄ±rtÄ±cÄ± derecede yalÄ±n. Modelin iÃ§inde olup bitenler aslÄ±nda hep aynÄ± tÃ¼r iÅŸlemlerden ibaret: matrisi Ã§arp, topla, normalize et ve tekrarla.

          PaylaÅŸtÄ±ÄŸÄ±m ÅŸu kÃ¼Ã§Ã¼k kod bloÄŸuna bir bakÄ±n:
        <div className="writingArticle__codeCard">
          <pre className="writingArticle__codeBlock">
            <code>
              <span className="codeLine">
                <span className="codeVar">prompt</span>
                <span className="codeEq"> = </span>
                <span className="codeString">"Ghosting yapmadÄ±ÄŸÄ±mÄ± anlatan ikna edici bir mesaj yaz"</span>
              </span>
              <br />
              <span className="codeLine">
                <span className="codeVar">tokens</span>
                <span className="codeEq"> = </span>
                <span className="codeFn">tokenizer</span>
                <span className="codeOp">(</span>
                <span className="codeVar">prompt</span>
                <span className="codeOp">);</span>
              </span>
              <span className="codeLine">
                <span className="codeKw">while</span>
                <span className="codeOp"> (</span>
                <span className="codeConst">true</span>
                <span className="codeOp">) &#123;</span>
              </span>
              <span className="codeLine">
                <span className="codeVar">  embeddings</span>
                <span className="codeEq"> = </span>
                <span className="codeFn">embed</span>
                <span className="codeOp">(</span>
                <span className="codeVar">tokens</span>
                <span className="codeOp">);</span>
              </span>
              <span className="codeLine">
                <span className="codeKw">  for</span>
                <span className="codeOp"> ([</span>
                <span className="codeVar">attention</span>
                <span className="codeOp">, </span>
                <span className="codeVar">feedforward</span>
                <span className="codeOp">] of </span>
                <span className="codeVar">transformers</span>
                <span className="codeOp">) &#123;</span>
              </span>
              <span className="codeLine">
                <span className="codeVar">    embeddings</span>
                <span className="codeEq"> = </span>
                <span className="codeFn">attention</span>
                <span className="codeOp">(</span>
                <span className="codeVar">embeddings</span>
                <span className="codeOp">);</span>
              </span>
              <span className="codeLine">
                <span className="codeVar">    embeddings</span>
                <span className="codeEq"> = </span>
                <span className="codeFn">feedforward</span>
                <span className="codeOp">(</span>
                <span className="codeVar">embeddings</span>
                <span className="codeOp">);</span>
              </span>
              <span className="codeLine">
                <span className="codeOp">  &#125;</span>
              </span>
              <span className="codeLine">
                <span className="codeVar">  output_token</span>
                <span className="codeEq"> = </span>
                <span className="codeFn">output</span>
                <span className="codeOp">(</span>
                <span className="codeVar">embeddings</span>
                <span className="codeOp">);</span>
              </span>
              <span className="codeLine">
                <span className="codeKw">  if</span>
                <span className="codeOp"> (</span>
                <span className="codeVar">output_token</span>
                <span className="codeEq"> === </span>
                <span className="codeConst">END_TOKEN</span>
                <span className="codeOp">) &#123;</span>
              </span>
              <span className="codeLine">
                <span className="codeKw">    break</span>
                <span className="codeOp">;</span>
              </span>
              <span className="codeLine">
                <span className="codeOp">  &#125;</span>
              </span>
              <span className="codeLine">
                <span className="codeVar">  tokens</span>
                <span className="codeOp">.</span>
                <span className="codeFn">push</span>
                <span className="codeOp">(</span>
                <span className="codeVar">output_token</span>
                <span className="codeOp">);</span>
              </span>
              <span className="codeLine">
                <span className="codeOp">&#125;</span>
              </span>
              <br />
              <span className="codeLine">
                <span className="codeFn">print</span>
                <span className="codeOp">(</span>
                <span className="codeFn">decode</span>
                <span className="codeOp">(</span>
                <span className="codeVar">tokens</span>
                <span className="codeOp">));</span>
              </span>
            </code>
          </pre>
        </div>
        
      DÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼mÃ¼z kadar karmaÅŸÄ±k kod yÄ±ÄŸÄ±nlarÄ±ndan bahsetmiyoruz. Hatta bazÄ± aÃ§Ä±k kaynaklÄ± modellerin Ã§alÄ±ÅŸan Ã¶rnekleri birkaÃ§ yÃ¼z satÄ±r Python koduyla ayaÄŸa kaldÄ±rÄ±labiliyor. <a href="https://magazine.sebastianraschka.com/" target="_blank" rel="noreferrer">Sebastian Raschka</a> gibi araÅŸtÄ±rmacÄ±larÄ±n PyTorch kullanarak bu mimarileri sÄ±fÄ±rdan kurduÄŸu o sade ve Ã¶ÄŸretici Ã¶rnekler de bu zarafetin en gÃ¼zel kanÄ±tÄ±.
         
         Mimarinin bu kadar yalÄ±n gÃ¶rÃ¼nmesi, yapÄ±lan iÅŸin kÃ¼Ã§Ã¼k olduÄŸu anlamÄ±na gelmiyor; tam tersine, bu sadelik yapÄ±lan iÅŸin ne kadar devasa olduÄŸunu daha net hissettiriyor. Modelin attÄ±ÄŸÄ± her kÃ¼Ã§Ã¼cÃ¼k adÄ±m, aslÄ±nda milyonlarca, hatta milyarlarca sayÄ±nÄ±n aynÄ± anda birbirini etkilemesi demek.
         
         <Note>
           <span className="writingNote__icon" aria-hidden="true">!</span>
           <strong>Perde arkasÄ±ndaki dev maraton: Neden bu kadar Ã§ok hesaplama?</strong>
           <br />
          Kendiniz deneyimleyin: Bu maratonun nasÄ±l iÅŸlediÄŸini (her bir headâ€™in kelimelere nasÄ±l baktÄ±ÄŸÄ±nÄ± ve olasÄ±lÄ±klarÄ±n nasÄ±l hesaplandÄ±ÄŸÄ±nÄ±, vb.) interaktif olarak gÃ¶rmek isterseniz <a className="writingArticle__link" href="https://poloclub.github.io/transformer-explainer/" target="_blank" rel="noreferrer">Transformer Explainer</a> aracÄ±nÄ± mutlaka kurcalayÄ±n. SayÄ±lar arasÄ±ndaki o Ã§izgilerin nasÄ±l canlandÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼ÄŸÃ¼nÃ¼zde taÅŸlar tam olarak yerine oturacak.

           **AÃ§Ä±klama:**

           Bu hesaplar Ã¶yle bir kez yapÄ±lÄ±p bitmiyor. Siz tek bir cÃ¼mle kurarken bile, o meÅŸhur Transformer katmanlarÄ± aslÄ±nda kendi iÃ§inde devasa bir maraton koÅŸuyor. Bu sÃ¼reci gÃ¶rseldeki detaylarla anlamlandÄ±rÄ±rsak:
           <ul className="writingArticle__ArchitectureList">
             <li>**Dikey TÄ±rmanÄ±ÅŸ (Blocks):** Modelin iÃ§inde tek bir iÅŸlem katmanÄ± yok. Transformer Explainer sayfasÄ±nÄ±n saÄŸ Ã¼stÃ¼nde gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z o "11 more identical Transformer Blocks" ifadesi, verinin aynÄ± iÅŸlemlerden onlarca kez (bazÄ± modellerde 96 katmana kadar Ã§Ä±kar) Ã¼st Ã¼ste geÃ§mesi demek.</li>
             <li>**Paralel BakÄ±ÅŸ AÃ§Ä±larÄ± (Heads):** Her bir blokta, sayfadaki gibi birden fazla "Head" aynÄ± anda Ã§alÄ±ÅŸÄ±yor. Kimi dil bilgisine, kimi kelimeler arasÄ± mesafeye, kimi de anlam derinliÄŸine odaklanÄ±yor. Tek bir blokta bazen onlarca farklÄ± "kafa" aynÄ± anda hesaplama yapÄ±yor.</li>
             <li>**AdÄ±m AdÄ±m Ãœretim (Autoregressive):** En kritik dÃ¶ngÃ¼ ise ÅŸu; model tek seferde bÃ¼tÃ¼n bir cÃ¼mreyi Ã¼retmiyor. Her bir kelimeyi (token) bulabilmek iÃ§in tÃ¼m bu katmanlarÄ± ve "head"leri en baÅŸtan (veya Ã¶nceki bilgileri tazeleyerek) tekrar tekrar kullanÄ±yor.</li>
           </ul>
           Ä°ÅŸte bu yÃ¼zden LLMâ€™lerde "devasa" olan ÅŸey algoritmanÄ±n iÃ§inden Ã§Ä±kÄ±lmaz karmaÅŸÄ±klÄ±ÄŸÄ± deÄŸil; bu basit adÄ±mlarÄ±n (Ã§arpma ve toplama) Ã§ok fazla kere tekrarlanmasÄ±dÄ±r. Gereken bu devasa iÅŸlem gÃ¼cÃ¼ ve enerji maliyeti, bÃ¼yÃ¼k teknoloji ÅŸirketlerinin neden yeni nÃ¼kleer enerji anlaÅŸmalarÄ± peÅŸinde koÅŸtuÄŸunu da Ã§ok iyi aÃ§Ä±klÄ±yor.
         </Note>


      ArtÄ±k bÃ¼yÃ¼k resmi gÃ¶rdÃ¼ÄŸÃ¼mÃ¼ze gÃ¶re, gelin bu Ã¼retim bandÄ±nÄ±n en baÅŸÄ±na, yani sizin ChatGPT'ye yazdÄ±ÄŸÄ±nÄ±z o metnin bu matematik dÃ¼nyasÄ±nÄ±n anlayacaÄŸÄ± dile nasÄ±l tercÃ¼me edildiÄŸine bakalÄ±m.

     Sahne sÄ±rasÄ± fabrikamÄ±zÄ±n ilk duraÄŸÄ±nda: **Tokenizer**.

       <h2>Tokenizer</h2>

       Bir yapay zekÃ¢ya "Naber?" dediÄŸinizde, aslÄ±nda o "Naber?" kelimesini bizim anladÄ±ÄŸÄ±mÄ±z anlamda gÃ¶rmez. Modelin kalbindeki o devasa matematiksel sistem, harflerle deÄŸil sayÄ±larla konuÅŸur. Bu yÃ¼zden yazdÄ±ÄŸÄ±nÄ±z her metnin, modelin iÅŸlemcilerine girmeden Ã¶nce onun ana diline Ã§evrilmesi gerekir. Tokenizer'Ä±n gÃ¶revi tam olarak budur: Ä°nsan dilini, modelin Ã¼zerinde hesap yapabileceÄŸi atomik parÃ§alara ayÄ±rmak.
       
       Ancak bu dÃ¶nÃ¼ÅŸÃ¼m sandÄ±ÄŸÄ±mÄ±z kadar "kelime bazlÄ±" iÅŸlemiyor. Biz gÃ¼nlÃ¼k hayatta dili kelimeler Ã¼zerinden dÃ¼ÅŸÃ¼nÃ¼rÃ¼z; aralara boÅŸluklar koyar, kelimeleri net sÄ±nÄ±rlar olarak gÃ¶rÃ¼rÃ¼z. Ama bir model iÃ§in bu yaklaÅŸÄ±m pek de akÄ±llÄ±ca deÄŸil. EÄŸer her ÅŸeyi kelime kelime ele alsaydÄ±; "kitap", "kitaplar", "kitapÃ§Ä±" gibi her tÃ¼revi ayrÄ± ayrÄ± ezberlemesi gerekirdi. Bu hem sÃ¶zlÃ¼ÄŸÃ¼ (vocabulary) gereksiz yere ÅŸiÅŸirir hem de modelin yeni kelimeler karÅŸÄ±sÄ±nda "Ã§aresiz" kalmasÄ±na neden olur.
       
       Ä°ÅŸte bu yÃ¼zden gÃ¼nÃ¼mÃ¼z modelleri **Subword Tokenization** dediÄŸimiz bir yÃ¶ntem kullanÄ±yor. Yani "kitapÃ§Ä±lÄ±k" kelimesini tek parÃ§a olarak deÄŸil, anlamlÄ± alt parÃ§alarÄ±na ayÄ±rÄ±yor. Bu konu o kadar derin ve kritik ki, yapay zekÃ¢ dÃ¼nyasÄ±nÄ±n efsane isimlerinden <a href="https://karpathy.ai/" target="_blank" rel="noreferrer">Andrej Karpathy</a>'nin sadece sÄ±fÄ±rdan bir tokenizer inÅŸa etmek Ã¼zerine saatler sÃ¼ren meÅŸhur bir <a href="https://www.youtube.com/watch?v=zduSFxRajkE/" target="_blank" rel="noreferrer">videosu</a> var.
       
      Bizim iÃ§in ÅŸimdilik ÅŸu Ã¶zet yeterli: Tokenizer, kelime kadar anlamlÄ± ama harf kadar esnek olan o "altÄ±n ortayÄ±" buluyor. Metni Ã¶yle bir parÃ§alÄ±yor ki, model hem bildiÄŸi kÃ¶klerden anlam Ã¼retebiliyor hem de daha Ã¶nce hiÃ§ gÃ¶rmediÄŸi bir kelimeyle karÅŸÄ±laÅŸtÄ±ÄŸÄ±nda onu tanÄ±dÄ±k parÃ§alara bÃ¶lerek bir mantÄ±k yÃ¼rÃ¼tebiliyor.

      <h3>**Bir "metin bÃ¶lÃ¼cÃ¼"den Ã§ok daha fazlasÄ±**</h3>

     Peki, bizim konuÅŸtuÄŸumuz dildeki o canlÄ± metinler, modelin soÄŸuk ve hesaplanabilir dÃ¼nyasÄ±ndaki bu parÃ§alara tam olarak nasÄ±l dÃ¶nÃ¼ÅŸÃ¼yor? Cevap ilk bakÄ±ÅŸta basit: Metni bir Tokenizerâ€™dan geÃ§iriyoruz.

     Ancak burada Ã§ok kritik bir nÃ¼ans var: Tokenizer, Ã¶yle rastgele Ã§alÄ±ÅŸan masum bir metin bÃ¶lÃ¼cÃ¼ deÄŸil. O da tÄ±pkÄ± modelin kendisi gibi, *Ã¶nceden eÄŸitilmiÅŸ* bir sistem. EÄŸitimi sÄ±rasÄ±nda devasa metin yÄ±ÄŸÄ±nlarÄ±nÄ± adeta bir dedektif gibi inceliyor; hangi harf dizilerinin yan yana gelmeyi sevdiÄŸini Ã¶ÄŸreniyor. TÃ¼rkÃ§eâ€™deki "-iyor" eki, Ä°ngilizceâ€™deki "the" kelimesi veya teknik terimlerdeki "ing" gibi parÃ§alar bu ÅŸekilde "anlamlÄ± birimler" olarak kaydediliyor. EÄŸitim bittiÄŸinde ise elinde on binlerce parÃ§adan oluÅŸan sabit ve deÄŸiÅŸmez bir liste kalÄ±yor.
       
    Bu yÃ¼zden Tokenizer daha Ã¶nce hiÃ§ gÃ¶rmediÄŸi yeni bir kelimeyle karÅŸÄ±laÅŸtÄ±ÄŸÄ±nda "Dur ÅŸunu sÃ¶zlÃ¼ÄŸÃ¼me ekleyeyim" demez. Onu, halihazÄ±rda bildiÄŸi parÃ§alardan en mantÄ±klÄ± ÅŸekilde nasÄ±l bÃ¶lebileceÄŸine bakar. TÄ±pkÄ± TÃ¼rkÃ§e bilmeyen birinin uzun bir kelimeyi hecelerken zorlanmasÄ± gibi, Tokenizer da sadece eÄŸitildiÄŸi dÃ¼nyanÄ±n kurallarÄ±yla hareket eder.
       
       Siz bir "prompt" girdiÄŸinizde, arka planda ÅŸu Ã¼Ã§ aÅŸamalÄ± operasyon gerÃ§ekleÅŸir:

          <ul className="writingArticle__ArchitectureList">
           <li>**ParÃ§alama:** Metni bildiÄŸi en uygun ve anlamlÄ± alt parÃ§alara (sub-tokens) ayÄ±rÄ±r.</li>
           <li>**EÅŸleme:** Her parÃ§ayÄ± o meÅŸhur sÃ¶zlÃ¼ÄŸÃ¼ndeki (vocabulary) doÄŸru yere yerleÅŸtirir.</li>
           <li>**ID Ãœretimi:** Ve her parÃ§aya karÅŸÄ±lÄ±k gelen, modelin asÄ±l iÅŸlem yapacaÄŸÄ± o benzersiz Token ID'yi Ã¼retir</li>
         </ul>

         ğŸ’¡ Kendiniz deneyimleyin: EÄŸer bu sÃ¼recin sizin yazdÄ±ÄŸÄ±nÄ±z cÃ¼mlelerde nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± canlÄ± canlÄ± gÃ¶rmek isterseniz, OpenAIâ€™Ä±n online <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noreferrer">tokenizer aracÄ±nÄ±</a> mutlaka kurcalayÄ±n. YazdÄ±ÄŸÄ±nÄ±z her kelimenin nasÄ±l renkli bloklara ve sayÄ±sal kimliklere bÃ¼rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ gÃ¶rmek, meselenin mantÄ±ÄŸÄ±nÄ± kavramayÄ± inanÄ±lmaz kolaylaÅŸtÄ±rÄ±yor.
         
        <h2>Embeddings</h2>

        Geldik iÅŸin mutfaÄŸÄ±na. Tokenizerâ€™dan Ã§Ä±kan o IDâ€™ler model iÃ§in aslÄ±nda hala sadece birer "numaradan" ibaret. Bu kuru rakamlarla tek baÅŸÄ±na yapabileceÄŸimiz ÅŸey ise maalesef Ã§ok kÄ±sÄ±tlÄ±. Ne kelimelerin benzerliÄŸini Ã¶lÃ§ebiliriz ne de "Acaba bunlar birbirine yakÄ±n mÄ±?" diye sorabiliriz. AnlamÄ±n gerÃ§ekten inÅŸa edilebilmesi iÃ§in, bu sayÄ±larÄ±n birbirleriyle kÄ±yaslanabilir, yani matematiksel bir ruhu olan forma dÃ¶nÃ¼ÅŸmesi ÅŸart. Ä°ÅŸte Embeddings katmanÄ± tam olarak bu dÃ¶nÃ¼ÅŸÃ¼mÃ¼ saÄŸlÄ±yor.
        
       OlayÄ± ÅŸÃ¶yle hayal edebilirsiniz: Her Token ID, modelin iÃ§inde saklanan devasa bir sÃ¶zlÃ¼k tablosunda (embedding table) bir satÄ±ra karÅŸÄ±lÄ±k geliyor. O satÄ±rdan Ã§ekip aldÄ±ÄŸÄ±mÄ±z o upuzun sayÄ± listesi, artÄ±k o kelimenin Ã§ok boyutlu bir vektÃ¶r olarak temsil edilmesini saÄŸlÄ±yor.

       Tokenizerâ€™dan Ã§Ä±kan Token IDâ€™ler, model iÃ§in hÃ¢lÃ¢ sadece numaralardan ibaret. Bu numaralarla tek baÅŸÄ±na yapÄ±labilecek ÅŸey Ã§ok sÄ±nÄ±rlÄ±. Ne benzerlik Ã¶lÃ§ebilirsin, ne â€œbunlar birbirine yakÄ±n mÄ±?â€ diye sorabilirsiniz. AnlamÄ±n inÅŸa edilebilmesi iÃ§in, bu etiketlerin karÅŸÄ±laÅŸtÄ±rÄ±labilir bir forma dÃ¶nÃ¼ÅŸmesi gerekir. Embeddings katmanÄ±nÄ±n yaptÄ±ÄŸÄ± ÅŸey tam olarak budur.
       
       Peki, nedir bu vektÃ¶r? Bir embeddingâ€™i, devasa bir uzaydaki koordinat gibi dÃ¼ÅŸÃ¼nebilirsiniz. Mesela sadece 3 boyutlu bir dÃ¼nyada, bir kelimenin embeddingâ€™i [10, 4, 2] gibi bir ÅŸey olurdu; yani bizi x=10, y=4, z=2 noktasÄ±na gÃ¶tÃ¼rÃ¼rdÃ¼. Ama gerÃ§ekte bu uzay Ã¶yle Ã¼Ã§le beÅŸle sÄ±nÄ±rlÄ± deÄŸil, binlerce farklÄ± boyuta sahip! Kelimeler, bu uÃ§suz bucaksÄ±z uzayda anlamlarÄ±na gÃ¶re birbirlerine yaklaÅŸÄ±yor, uzaklaÅŸÄ±yor ya da kÃ¼meleniyorlar.
       
       <div className="writingArticle__callout">
           <div className="writingArticle__calloutTitle">Peki neden bu kadar Ã§ok boyut?</div>
           <div className="writingArticle__calloutBody">
           BÃ¼yÃ¼k dil modellerinin (LLM) elinde kelimelerin nasÄ±l eÅŸleÅŸeceÄŸini sÃ¶yleyen hazÄ±r bir rehber yok. Onlar dildeki o devasa Ã¶rÃ¼ntÃ¼leri ve gizli iliÅŸkileri sÄ±fÄ±rdan keÅŸfetmek zorunda.

         Bir cÃ¼mleyi sadece "ne dendiÄŸi" Ã¼zerinden deÄŸil; tonu, ritmi, resmi mi yoksa mahalle aÄŸzÄ± mÄ± olduÄŸu gibi binlerce farklÄ± sinyal Ã¼zerinden okuyorlar. Bu yÃ¼zden her bir kelime (token), tek bir Ã§izgi Ã¼zerinde deÄŸil, binlerce farklÄ± eksende aynÄ± anda temsil ediliyor.

     Ama iÅŸin en ilginÃ§ ve biraz da "garip" kÄ±smÄ± burasÄ±. Bu boyutlarÄ±n hiÃ§biri biz insanlar iÃ§in "Hah, iÅŸte bu neÅŸe demek" gibi net bir anlam taÅŸÄ±mÄ±yor. Modelin iÃ§inde "bu boyut resmiyeti Ã¶lÃ§er" veya "ÅŸu eksen ironiyi temsil eder" gibi etiketler yok. Bu ayrÄ±mlar modelin derinliklerinde var, tÄ±kÄ±r tÄ±kÄ±r Ã§alÄ±ÅŸÄ±yor ve sonuÃ§ veriyor; ama bizim dÃ¼nyamÄ±za doÄŸrudan tercÃ¼me edilemiyor.

    GÃ¼ncel modellerde bu boyutlar binlerle ifade ediliyor. Yani her kelime tek bir anlam deÄŸil, sayÄ±sÄ±z Ã¶zelliÄŸin Ã¼st Ã¼ste binmesiyle oluÅŸan bir "koordinatta" yaÅŸÄ±yor. Anlam dediÄŸimiz ÅŸey de o uzaydaki tek bir noktada deÄŸil, o devasa iliÅŸkiler aÄŸÄ±nÄ±n iÃ§inde doÄŸuyor.

    Modellerin en "bÃ¼yÃ¼lÃ¼" tarafÄ± belki de bu: Matematiksel adÄ±mlarla ilerliyorlar ama oluÅŸturduklarÄ± dÃ¼nya bizim dilimize tam olarak Ã§evrilemeyecek kadar derin ve Ã§ok boyutlu.
          </div>
         </div>

      **SayÄ±larÄ±n koordinata dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼ yer**

      GÃ¶rseldeki tabloya bir gÃ¶z atalÄ±m: Buradaki her bir satÄ±r, aslÄ±nda bir kelimenin (tokenâ€™Ä±n) modelin iÃ§indeki devasa "anlam sÃ¶zlÃ¼ÄŸÃ¼nden" aldÄ±ÄŸÄ± karÅŸÄ±lÄ±ÄŸÄ± gÃ¶steriyor. Her bir sÃ¼tun ise o kelimenin karakterini belirleyen farklÄ± bir boyutu temsil ediyor.       
        <div className="writingArticle__embedVisualization">
         <div className="writingArticle__embedTable">
         <div className="writingArticle__embedTableHeader">
           <span>Token ID</span>
           <span>d1</span>
           <span>d2</span>
           <span>d3</span>
         </div>
         <div className="writingArticle__embedTableRow">
           <span className="writingArticle__embedTableId">684</span>
           <span>-11.1</span>
           <span>7.3</span>
           <span>-5.1</span>
         </div>
         <div className="writingArticle__embedTableRow">
           <span className="writingArticle__embedTableId">704</span>
           <span>8.6</span>
           <span>-5.4</span>
           <span>9.5</span>
         </div>
         <div className="writingArticle__embedTableRow">
           <span className="writingArticle__embedTableId">73963</span>
           <span>-3.2</span>
           <span>6.1</span>
           <span>0.7</span>
         </div>
        <div className="writingArticle__embedTableRow">
          <span className="writingArticle__embedTableId">10709</span>
          <span>-10.1</span>
          <span>-1.4</span>
          <span>-8.9</span>
        </div>
       </div>
       <Scatterplot3DDemo 
        id={'embeddings'}
        pointSets={[
          { name: 'Embeddings', points: [[-11.1, 7.3, -5.1], [8.6, -5.4, 9.5], [-3.2, 6.1, 0.7], [-10.1, -1.4, -8.9]]}
         ]}
        labels={["684", "704", "73963", "10709"]}
       />
       </div>
             SÃ¼reci adÄ±m adÄ±m hayal edersek:

        <ol className="writingArticle__TokenizerList writingArticle__TokenizerList--ordered">
         <li>**AdÄ±m (Token'lar):** Elimizde Tokenizerâ€™dan gelen tam sayÄ±lar var. Ama bunlar sadece birer etiket. AralarÄ±nda mesafe, benzerlik veya anlam gibi hiÃ§bir baÄŸ yok; model iÃ§in hepsi birbirinden baÄŸÄ±msÄ±z numaralar.</li>
         <li>**AdÄ±m (Embedding'ler):** Sihir burada baÅŸlÄ±yor. Her sayÄ±, modelin eÄŸitimi sÄ±rasÄ±nda ilmek ilmek iÅŸlenmiÅŸ o devasa tablodan kendi satÄ±rÄ±nÄ± buluyor. ArtÄ±k elimizde tek bir rakam deÄŸil; o kelimenin Ã§ok boyutlu uzaydaki yerini belirleyen ondalÄ±klÄ± bir sayÄ± listesi, yani bir vektÃ¶r var.</li>
         </ol>

        <Note>
          <span className="writingNote__icon" aria-hidden="true">!</span>
          Bu embeddingâ€™ler ilk baÅŸta rastgele bir noktadan baÅŸlÄ±yor. Model eÄŸitildikÃ§e, hata payÄ±na gÃ¶re bu noktalarÄ±n yerini milim milim ayarlÄ±yor. SonuÃ§ta birbirine benzer baÄŸlamlarda kullanÄ±lan kelimeler bu uzayda yavaÅŸ yavaÅŸ yan yana gelmeye baÅŸlÄ±yor; alakasÄ±z olanlar ise birbirine uzaklaÅŸÄ±yor.
        </Note>

         **Statik Bir HaritadayÄ±z (Åimdilik!)**

         Bu noktada tokenâ€™lar ilk kez birbiriyle kÄ±yaslanabilir hale geliyor. Ancak burada karÄ±ÅŸtÄ±rÄ±lmamasÄ± gereken ince bir detay var: Anlam hÃ¢lÃ¢ doÄŸrudan ortaya Ã§Ä±kmÄ±ÅŸ deÄŸil, sadece anlamÄ±n kurulabileceÄŸi matematiksel zemin hazÄ±rlandÄ±.
         
         <Note>
           <span className="writingNote__icon" aria-hidden="true">!</span>
           "Kelimeler birbirine yaklaÅŸÄ±yor" dediÄŸimizde, bu modelin aylar sÃ¼ren eÄŸitim sÃ¼recinde gerÃ§ekleÅŸen bir durumdur. Model dili Ã¶ÄŸrenirken bu koordinatlarÄ± ayarlar. Ancak model Ã§alÄ±ÅŸÄ±rken (biz ona bir soru sorduÄŸumuzda), bu embedding tablosu artÄ±k sabittir.
         </Note>
         
         
         Buradaki kilit nokta; Embeddingâ€™ler kelimeye Ã¶zeldir, cÃ¼mleye deÄŸil. "Banka" kelimesi, tek baÅŸÄ±na ele alÄ±ndÄ±ÄŸÄ±nda sÃ¶zlÃ¼kten hep aynÄ± koordinatÄ± Ã§eker. Bu koordinat, kelimenin hangi baÄŸlamda kullanÄ±ldÄ±ÄŸÄ±nÄ± henÃ¼z ayÄ±rt edemez. "Parktaki banka oturdum" ile "MaaÅŸÄ±n yattÄ±ÄŸÄ± banka" arasÄ±ndaki farkÄ± model bu aÅŸamada hÃ¢lÃ¢ anlayamaz; Ã§Ã¼nkÃ¼ her iki cÃ¼mlede de "banka" kelimesi iÃ§in aynÄ± satÄ±rdaki aynÄ± sayÄ± listesini kullanÄ±r.
         
         Embeddings'in yaptÄ±ÄŸÄ± ÅŸey, tokenâ€™larÄ± devasa ama statik bir haritaya yerleÅŸtirmektir. Model bu noktada hÃ¢lÃ¢ ÅŸu kritik sorularÄ±n cevabÄ±nÄ± bilmez:
           <ul className="writingArticle__ArchitectureList">
           <li>"Bu cÃ¼mlede hangi kelime daha kritik?"</li>
           <li>"Åu an hangi kelimeye odaklanmalÄ±yÄ±m?"</li>
           <li>"YanÄ±ndaki kelime, bu kelimenin anlamÄ±nÄ± nasÄ±l deÄŸiÅŸtiriyor?"</li>
         </ul>

      Ä°ÅŸte tam bu noktada sahneye **Transformer** mimarisi Ã§Ä±kÄ±yor. Bu statik ve "ruhsuz" koordinatlarÄ± alÄ±p, **Attention (Dikkat)** mekanizmasÄ±yla birbirine Ã§arptÄ±rarak onlara can veren yer burasÄ±.            
       
       <h2 className="writingArticle__transformerTitle">Transformer</h2>

      Embeddings aÅŸamasÄ±nda kelimeleri sayÄ±lara Ã§evirdik ve her birini Ã§ok boyutlu bir haritadaki koordinatlarÄ±na yerleÅŸtirdik. Bu noktada her tokenâ€™Ä±n bir adresi var ama bu adres tek baÅŸÄ±na "anlam" demek deÄŸil. Ä°ÅŸte Transformer katmanÄ±nÄ±n gÃ¶revi burada baÅŸlÄ±yor: Bu ruhsuz ve statik vektÃ¶rleri alÄ±p, cÃ¼mledeki diÄŸer tokenâ€™larla etkileÅŸime sokmak ve onlarÄ± baÄŸlama gÃ¶re yeniden ÅŸekillendirmek. Yani yapay zekanÄ±n o meÅŸhur "anlamlandÄ±rma" yeteneÄŸi, ilk kez burada filizleniyor.

        ***Statik* vektÃ¶rlerden â†’ BaÄŸlamÄ± inÅŸa eden *dinamik* vektÃ¶rlere**
        
        Embedding aÅŸamasÄ±nda her kelime, cÃ¼mleden baÄŸÄ±msÄ±z olarak uzayda tek bir noktaya karÅŸÄ±lÄ±k geliyordu. Ã–rneÄŸin "banka" kelimesi, hangi cÃ¼mlede geÃ§erse geÃ§sin baÅŸlangÄ±Ã§ta aynÄ± koordinattaydÄ±. Ancak gerÃ§ek dil bÃ¶yle Ã§alÄ±ÅŸmaz:

        <ul className="writingArticle__ArchitectureList">
           <li>*"Parktaki eski bir banka oturup dinlendi."*</li>
           <li>*"Banka ÅŸubesine giderek yeni bir hesap aÃ§tÄ±."*</li>
         </ul>

         Bu iki cÃ¼mlede kelime aynÄ± olsa da anlamlar dÃ¼nyalar kadar farklÄ±dÄ±r. Transformerâ€™Ä±n yaptÄ±ÄŸÄ± sihir, bu farkÄ± matematiksel olarak gÃ¶rÃ¼nÃ¼r kÄ±lmaktÄ±r. Statik embeddingâ€™leri alÄ±r, cÃ¼mledeki diÄŸer kelimelerle bir nevi "konuÅŸturur" ve her tokenâ€™Ä± bulunduÄŸu baÄŸlama gÃ¶re haritada yeniden konumlandÄ±rÄ±r. ArtÄ±k her kelime sadece "ne olduÄŸu" ile deÄŸil, diÄŸer kelimelerle kurduÄŸu iliÅŸki Ã¼zerinden temsil edilir.

         Bu dÃ¶nÃ¼ÅŸÃ¼m birkaÃ§ farklÄ± mekanizma Ã¼zerinden yapÄ±lsa da, sistemin kalbinde tek bir altÄ±n kural var: **Her kelime eÅŸit derecede Ã¶nemli deÄŸildir.** Ä°ÅŸte tam bu noktada, modelin hangi kelimeye ne kadar odaklanacaÄŸÄ±nÄ± seÃ§tiÄŸi o meÅŸhur **Attention (Dikkat)** mekanizmasÄ± devreye girer.
        

       <h3 className="writingArticle__transformerTitle">Attention: Hangi token, ne kadar Ã¶nemli?</h3>

       Attention mekanizmasÄ± aslÄ±nda ÅŸu basit ama hayati soruyu sorar: "Bu cÃ¼mleyi doÄŸru anlamak iÃ§in, ÅŸu anda hangi tokenâ€™lara, ne kadar odaklanmalÄ±yÄ±m?"

       Model metni iÅŸlerken yine parÃ§a parÃ§a (token token) ilerler. Ancak buradaki kritik fark ÅŸudur: Model bir token'a geldiÄŸinde, geÃ§miÅŸteki tÃ¼m token'lara eÅŸit davranmaz. OnlarÄ± Ã¶nem derecelerine gÃ¶re tartar ve aÄŸÄ±rlÄ±klÄ± bir kombinasyon oluÅŸturur.

       Ã–rneÄŸin, *"ParayÄ± bankaya yatÄ±rdÄ±m"* cÃ¼mlesinde model "yatÄ±rdÄ±m" token'Ä±na ulaÅŸtÄ±ÄŸÄ±nda, Ã¶nceki parÃ§alarÄ± kafasÄ±nda kabaca ÅŸÃ¶yle aÄŸÄ±rlÄ±klandÄ±rabilir:
       <ul className="writingArticle__TokenizerList">
         <li>**%48 â†’ bankaya:** Eylemin tam olarak nereye yapÄ±ldÄ±ÄŸÄ±nÄ± belirlediÄŸi iÃ§in aslan payÄ± buraya gider.</li>
         <li>**%34 â†’ parayÄ±:** Neyin yatÄ±rÄ±ldÄ±ÄŸÄ±nÄ± sÃ¶ylediÄŸi iÃ§in ikinci kritik odak noktasÄ±dÄ±r.</li>
         <li>**%18 â†’ yatÄ±rdÄ±m Ã¶ncesi baÄŸlam:** Zaman, gramer ve akÄ±ÅŸ sinyalleri gibi yardÄ±mcÄ± unsurlar.</li>
       </ul>

       Model, bu token'larÄ±n embeddingâ€™lerini bu oranlarda harmanlayarak ortaya yeni bir **baÄŸlamsal vektÃ¶r** Ã§Ä±karÄ±r. Bu yeni vektÃ¶r, artÄ±k sadece "banka" kelimesini deÄŸil; "finansal bir iÅŸlem iÃ§in kullanÄ±lan banka" anlamÄ±nÄ± taÅŸÄ±r.

      <Note>
        <span className="writingNote__icon" aria-hidden="true">!</span>
        Bu dikkat sÃ¼reci sadece yanÄ±t Ã¼retirken Ã§alÄ±ÅŸmaz. Model, sizin yazdÄ±ÄŸÄ±nÄ±z promptâ€™u anlamaya Ã§alÄ±ÅŸÄ±rken de tam olarak aynÄ± mekanizmayÄ± Ã§alÄ±ÅŸtÄ±rÄ±r. Siz ne kadar net ve detaylÄ± bir Ã§erÃ§eve Ã§izerseniz, modelin kurduÄŸu o baÄŸlamsal aÄŸ da o kadar saÄŸlam olur.

      Tam da bu yÃ¼zden **Prompt Engineering** dediÄŸimiz uzmanlÄ±k dalÄ± doÄŸdu. YazdÄ±ÄŸÄ±nÄ±z her bir kelime, modelin "dikkatini" nereye odaklayacaÄŸÄ±na dair birer komut niteliÄŸinde. Yani promptlarÄ±nÄ±zÄ±n kalitesi, aslÄ±nda modelin iÃ§indeki o milyarlarca matris Ã§arpÄ±mÄ±nÄ±n ne kadar isabetli olacaÄŸÄ±nÄ± belirliyor.
      </Note>

       
       **Attention bunu matematiksel olarak nasÄ±l yapÄ±yor? â†’ Q, K, V**

       KÄ±sa bir matematik molasÄ± veriyoruz. Ama hemen belirteyim; amacÄ±mÄ±z formÃ¼llere boÄŸulmak deÄŸil, modelin bu "odaklanma" iÅŸini perde arkasÄ±nda nasÄ±l bir mantÄ±kla hesapladÄ±ÄŸÄ±nÄ± sezgisel olarak kavramak.

       Az Ã¶nce bahsettiÄŸimiz o "dikkat oranlarÄ±" (%48, %34...) modelin iÃ§ine vahiy yoluyla inmiyor. Her ÅŸey tamamen matematiksel iÅŸlemlerle, adÄ±m adÄ±m hesaplanÄ±yor. Åimdi bu sÃ¼reci basitleÅŸtirilmiÅŸ bir Ã¶rnek Ã¼zerinden gÃ¶relim:

       <ul className="writingArticle__TokenizerList">
         <li> **Promptâ€™umuz yine aynÄ± olsun:** â€œParayÄ± bankaya yatÄ±rdÄ±mâ€</li>
         <li>**Token IDâ€™ler hazÄ±r:** Tokenizer bu cÃ¼mleyi dÃ¶rt parÃ§aya bÃ¶ldÃ¼: [ParayÄ±] [bankaya] [yatÄ±r] [dÄ±m]</li>
         <li>**Embeddingâ€™ler:** GÃ¶rselleÅŸtirmeyi kolaylaÅŸtÄ±rmak iÃ§in n=3 boyutlu bir dÃ¼nyada olduÄŸumuzu hayal edelim. (GerÃ§ekte bu binlerce boyuttur ama mekanizmayÄ± anlamak iÃ§in bu mini matris bize yeter.)</li>
       </ul>
       
        <div className="writingArticle__codeRow">
         <div className="writingArticle__codeCard writingArticle__codeCard--narrow">
           <pre className="writingArticle__codeBlock">
             <code>
               <span className="codeLine">
                 <span className="codeConst">Embeddings</span>
                 <span className="codeOp"> (</span>
                 <span className="codeVar">4 Ã— 3</span>
                 <span className="codeOp">)</span>
               </span>
               <br />
               <br />
               <span className="codeLine ">
                <span className="codeConst codeLine--row-1">Token 1:</span>
                <span className="codeVar"> ParayÄ±</span>
                 <span className="codeOp">  â†’ </span>
                 <span className="codeConst codeLine--row-1">Embedding 1:</span>
                 <span className="codeOp">[ </span>
                 <span className="codeVar">0.2</span>
                 <span className="codeOp"> , </span>
                 <span className="codeVar">-0.1</span>
                 <span className="codeOp"> , </span>
                 <span className="codeVar">0.7</span>
                 <span className="codeOp"> ]</span>
               </span>
               <br />
               <span className="codeLine ">
                <span className="codeConst codeLine--row-2">Token 2:</span>
                <span className="codeVar"> bankaya</span>
                 <span className="codeOp"> â†’ </span>
                 <span className="codeConst codeLine--row-2">Embedding 2:</span>
                 <span className="codeOp">[ </span>
                 <span className="codeVar">0.9</span>
                 <span className="codeOp"> , </span>
                 <span className="codeVar">0.3</span>
                 <span className="codeOp"> , </span>
                 <span className="codeVar">-0.2</span>
                 <span className="codeOp"> ]</span>
               </span>
               <br />
               <span className="codeLine ">
                <span className="codeConst codeLine--row-3">Token 3:</span>
                <span className="codeVar"> yatÄ±r</span>
                 <span className="codeOp">   â†’ </span>
                 <span className="codeConst codeLine--row-3">Embedding 3:</span>
                 <span className="codeOp">[ </span>
                 <span className="codeVar">0.4</span>
                 <span className="codeOp"> , </span>
                 <span className="codeVar">0.8</span>
                 <span className="codeOp"> , </span>
                 <span className="codeVar">0.1</span>
                 <span className="codeOp"> ]</span>
               </span>
               <br />
               <span className="codeLine ">
                <span className="codeConst codeLine--row-4">Token 4:</span>
                <span className="codeVar"> dÄ±m</span>
                 <span className="codeOp">     â†’ </span>
                 <span className="codeConst codeLine--row-4">Embedding 4:</span>
                 <span className="codeOp">[ </span>
                 <span className="codeVar">-0.3</span>
                 <span className="codeOp">, </span>
                 <span className="codeVar">0.2</span>
                 <span className="codeOp"> , </span>
                 <span className="codeVar">0.6</span>
                 <span className="codeOp"> ]</span>
               </span>
             </code>
           </pre>
         </div>
         <div className="writingArticle__codeSymbol" aria-hidden="true">â†’</div>
         <Matrix values={[[0.2, -0.1, 0.7], [0.9, 0.3, -0.2], [0.4, 0.8, 0.1], [-0.3, 0.2, 0.6]]} title={{ text:"Embedding Matrix" }} />
        </div> 
       Bu noktada Attention mekanizmasÄ± devreye girer ve bu embeddingâ€™lere 3 farklÄ± lensten bakar. (Yani model, her bir embeddingâ€™i alÄ±r ve onu Ã¼Ã§ farklÄ± matematiksel dÃ¶nÃ¼ÅŸÃ¼mden geÃ§irir.) Bu Ã¼Ã§ dÃ¶nÃ¼ÅŸÃ¼me **Query (Sorgu)**, **Key (Anahtar)** ve **Value (DeÄŸer)** diyoruz.                      
        <div className="writingArticle__callout">

        **Q ve K karar verir, V aktarÄ±lÄ±r**

        Peki, bu Ã¼Ã§ farklÄ± bakÄ±ÅŸ aÃ§Ä±sÄ± (Q, K, V) pratikte ne anlama geliyor? Gelin, "bankaya" ve "yatÄ±rdÄ±m" token'larÄ± Ã¼zerinden bu matematiksel fÄ±sÄ±ldaÅŸmayÄ± tercÃ¼me edelim:

       <ol className="writingArticle__TokenizerList writingArticle__TokenizerList--ordered">
         <li>**Query (Q - Sorgu)** *â€œBu token, cÃ¼mleyi doÄŸru anlamak iÃ§in diÄŸer tokenâ€™larda hangi Ã¶zelliklere dikkat etmeli?â€*</li>

         Ã–rneÄŸin "yatÄ±rdÄ±m" token'Ä±nÄ± dÃ¼ÅŸÃ¼nÃ¼n. Bu token iÃ§in model ÅŸu sorularÄ±n peÅŸine dÃ¼ÅŸer: "Nereye yatÄ±rÄ±ldÄ±? Ne yatÄ±rÄ±ldÄ±? Bu eylemle iliÅŸkili bir yer var mÄ±?" Ä°ÅŸte bu "istek listesi" Query vektÃ¶rÃ¼dÃ¼r. Bu vektÃ¶r, â€œbankayaâ€ ve â€œparaâ€ gibi tokenâ€™larda gÃ¼Ã§lÃ¼ eÅŸleÅŸmeler bulursa, onlara daha fazla dikkat verilmesini saÄŸlar.

         <li>**Key (K - Anahtar)** *"Bu token, kendisiyle ilgili hangi sinyalleri dÄ±ÅŸarÄ±ya veriyor?"*</li>

         "bankaya" token'Ä± ise dÄ±ÅŸarÄ±ya ÅŸu etiketleri sunar: "Ben bir yer bilgisi iÃ§eriyorum, finansal bir baÄŸlam taÅŸÄ±yorum, eylemlerle iliÅŸkilendirilebilirim." Bu, kelimenin kÃ¼tÃ¼phane rafÄ±ndaki sÄ±rt etiketidir.

         <li>**Value (V - Value)** *"Bu token'dan baÄŸlama hangi bilgi taÅŸÄ±nmalÄ±?"*</li>
         </ol>

         Yani Q ve K, "Kime bakmalÄ±yÄ±m?" sorusunu Ã§Ã¶zer; V ise "BaktÄ±ysan, benden neyi alacaksÄ±n?" sorusunun cevabÄ±dÄ±r. "bankaya" token'Ä± burada finansal anlamÄ±nÄ± ve yer bilgisini paketleyip bir sonraki aÅŸamaya iletir. V (Value) vektÃ¶rÃ¼, bu taÅŸÄ±nabilir bilginin matematiksel temsilidir.

       **Matematiksel DÃ¶nÃ¼ÅŸÃ¼m: W<sub>Q</sub>, W<sub>K</sub> ve W<sub>V</sub> Matrisleri**

       Bu $\mathit{Q}$, $\mathit{K}$ ve $\mathit{V}$ vektÃ¶rleri gÃ¶kten inmiyor. Bunlar, elimizdeki **Embedding** matrisinin Ã¼Ã§ farklÄ± "Ã¶ÄŸrenilmiÅŸ matris" ile Ã§arpÄ±lmasÄ±yla elde ediliyor:

        <div className="writingArticle__mathLines">
          <div className="writingArticle__mathLine">$Q = Embeddings \times W_{Q}$</div>
          <div className="writingArticle__mathLine">$K = Embeddings \times W_{K}$</div>
          <div className="writingArticle__mathLine">$V = Embeddings \times W_{V}$</div>
        </div>


       Buradaki $\mathit{W\scriptsize{Q}}$, $\mathit{W\scriptsize{K}}$ ve $\mathit{W\scriptsize{V}}$  matrisleri, modelin eÄŸitimi sÄ±rasÄ±nda Ã¶ÄŸrendiÄŸi asÄ±l parametreler. BaÅŸlangÄ±Ã§ta bu matrislerin iÃ§i tamamen rastgele sayÄ±larla doludur; Ã§Ã¼nkÃ¼ model en baÅŸta herhangi bir dil kuralÄ± bilmez; zaten bilmesine gerek de yoktur. Modelin herhangi bir Ã¶n kabulle kÄ±sÄ±tlanmamasÄ±, dildeki kalÄ±plarÄ± ve kurallarÄ± tamamen kendi deneyimiyle, Ã¶zgÃ¼rce keÅŸfetmesini saÄŸlar. Ve eÄŸitim boyunca, Ã¼rettiÄŸi Ã§Ä±ktÄ±yÄ± doÄŸru cevapla karÅŸÄ±laÅŸtÄ±rÄ±r, hatasÄ±nÄ± Ã¶lÃ§er ve bu hataya gÃ¶re matrislerin iÃ§indeki sayÄ±larÄ± milim milim gÃ¼nceller. Milyarlarca Ã¶rnekten sonra bu matrisler artÄ±k ÅŸu sorulara harika cevap verir hale gelir:

        <ul className="writingArticle__TokenizerList">
         <li>"Hangi Ã¶zellikler birbiriyle gerÃ§ekten ilgili?"</li>
         <li>"Hangi sinyaller baÄŸlam iÃ§in Ã¶nemli, hangileri gÃ¼rÃ¼ltÃ¼?"</li>
       </ul>

       SÄ±kÃ§a duyduÄŸumuz "175 milyar parametre" kavramÄ± aslÄ±nda bu matrislerin iÃ§indeki o kÃ¼Ã§Ã¼k ondalÄ±klÄ± sayÄ±lardÄ±r. Her biri tek baÅŸÄ±na bir ÅŸey ifade etmez ama birlikte Ã§alÄ±ÅŸtÄ±klarÄ±nda, tÄ±pkÄ± beynimizdeki nÃ¶ronlar gibi, devasa bir davranÄ±ÅŸ bÃ¼tÃ¼nÃ¼ ve "anlam" oluÅŸtururlar.

       </div>

       **Åimdi matematik konuÅŸsun: Q ve K matrislerini hesaplayalÄ±m**

      Bu aÅŸamada yaptÄ±ÄŸÄ±mÄ±z ÅŸey henÃ¼z bir dikkat (attention) hesaplamak deÄŸil; her tokenâ€™Ä±n "aradÄ±ÄŸÄ±nÄ±" ($Q$) ve "sunduÄŸunu" ($K$) iki ayrÄ± matematiksel dÃ¶nÃ¼ÅŸÃ¼mle aynÄ± evrensel uzayda ifade etmek.

       <div className="writingArticle__codeRow">
         <Matrix values={[[0.2, -0.1, 0.7], [0.9, 0.3, -0.2], [0.4, 0.8, 0.1], [-0.3, 0.2, 0.6]]} title={{ text:"embeddings" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">Ã—</div>
         <Matrix values={[[0.5, -0.4, 0.1], [0.3, 0.8, -0.4], [-0.6, 0.4, 0.9]]} title={{ text:"WQ" }} />
         <div className="writingArticle__codeSymbol" aria-hidden="true">=</div>
         <Matrix values={[[-0.4, 0.1, 0.7], [0.7, -0.2, -0.2], [0.4, 0.5, -0.2], [-0.5, 0.5, 0.4]]} title={{ text:"Q"}}/>
       </div>

        <div className="writingArticle__codeRow">
         <Matrix values={[[0.2, -0.1, 0.7], [0.9, 0.3, -0.2], [0.4, 0.8, 0.1], [-0.3, 0.2, 0.6]]} title={{ text:"embeddings" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">Ã—</div>
         <Matrix values={[[0.1, -0.7, -0.6], [-0.8, 0.5, -0.8], [-0.3, 0.2, 0.9]]} title={{ text:"WK" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">=</div>
         <Matrix values={[[-0.1, -0.1, 0.6], [-0.1, -0.5, -1.0], [-0.6, 0.1, -0.8], [-0.4, 0.4, 0.6]]} title={{ text:"K" }}/>
       </div>

       Matrislere bakarsanÄ±z, $\mathit{Q}$ ve $\mathit{K}$ matrislerinin boyutlarÄ± bilinÃ§li olarak aynÄ± tutulur. Sebebi; birazdan bu iki temsili doÄŸrudan birbiriyle matematiksel bir "tokalaÅŸmaya" sokacaÄŸÄ±z. Yani bu adÄ±mda henÃ¼z "Kim kime ne kadar dikkat ediyor?" sorusunu sormuyoruz, sadece bu soruyu sorabilmek iÃ§in gerekli altyapÄ±yÄ± kuruyoruz.
              
       **$Q$ ve $K$ nihayet karÅŸÄ± karÅŸÄ±ya: Matrislerin tokalaÅŸmasÄ±**

       AltyapÄ±yÄ± kurduk; her token iÃ§in elimizde ne aradÄ±ÄŸÄ±nÄ± bilen bir Query ($Q$) ve ne sunduÄŸunu sÃ¶yleyen bir Key ($K$) var. Åimdi asÄ±l bÃ¼yÃ¼leyici kÄ±sma geliyoruz: Bu iki matrisin birbiriyle Ã§arpÄ±lmasÄ±yla elde edilen **Attention Scores (Dikkat SkorlarÄ±)**.

       Matematiksel olarak yaptÄ±ÄŸÄ±mÄ±z ÅŸey, $Q$ matrisi ile $K$ matrisinin transpozunu Ã§arpmaktÄ±r ($Q \times K^T$). Bu iÅŸlem aslÄ±nda ÅŸu anlama gelir: Model, "A kelimesinin aradÄ±ÄŸÄ± Ã¶zellikler, B kelimesinin sunduÄŸu etiketlerle ne kadar Ã¶rtÃ¼ÅŸÃ¼yor?" sorusuna her bir kelime Ã§ifti iÃ§in sayÄ±sal bir puan verir.

       Bu Ã§arpÄ±m sonucunda karÅŸÄ±mÄ±za her kelimenin diÄŸer her kelimeyle olan "alaka dÃ¼zeyini" gÃ¶steren dev bir tablo (skor matrisi) Ã§Ä±kar.
         
         <div className="writingArticle__codeRow">
         <Matrix values={[[-0.4, 0.1, 0.7], [0.7, -0.2, -0.2], [0.4, 0.5, -0.2], [-0.5, 0.5, 0.4]]} title={{ text:"Q" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">Ã—</div>
         <Matrix values={[[-0.1, -0.1, -0.6, -0.4], [-0.1, -0.5, 0.1, 0.4], [0.6, -0.1, -0.8, 0.6]]} title={{ text:"transpose (Káµ€)" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">=</div>
         <Matrix values={[[0.5, -0.1, -0.3, 0.6], [-0.2, 0.1, -0.3, -0.5], [-0.2, -0.3, 0.0, -0.1], [0.2, -0.2, 0.0, 0.6]]} title={{ text:"scores" }}/>
       </div>

       <ul className="writingArticle__TokenizerList">  
         <li>**SatÄ±rlar ve SÃ¼tunlar** - Scores tablosundaki her bir satÄ±r "ÅŸu anki tokenâ€™Ä±", her bir sÃ¼tun ise "bakÄ±lan tokenâ€™Ä±" temsil eder. Yani tabloya baktÄ±ÄŸÄ±nÄ±zda, modelin o satÄ±rdaki kelimeyi anlamlandÄ±rmak iÃ§in diÄŸer sÃ¼tunlardaki kelimelere ne kadar "danÄ±ÅŸtÄ±ÄŸÄ±nÄ±" gÃ¶rÃ¼rsÃ¼nÃ¼z.</li>
          <li>**Noktasal Ã‡arpÄ±m (Dot Product)** - Tablodaki her bir hÃ¼cre, ilgili iki vektÃ¶rÃ¼n ($Q$ ve $K$) noktasal Ã§arpÄ±mÄ±dÄ±r. Bu iÅŸlem, iki vektÃ¶rÃ¼n uzayda ne kadar aynÄ± yÃ¶ne baktÄ±ÄŸÄ±nÄ± Ã¶lÃ§er; yani matematiksel bir "hizalanma" skorudur.</li>
         <li>**Skorlar ne anlatÄ±yor?** - AslÄ±nda burada yaptÄ±ÄŸÄ±mÄ±z ÅŸey, tokenâ€™lar arasÄ± bir "uyum testi". Ortaya Ã§Ä±kan sayÄ± ne kadar bÃ¼yÃ¼kse, iki kelime arasÄ±ndaki baÄŸ o kadar gÃ¼Ã§lÃ¼dÃ¼r. Ã–rneÄŸin; "yatÄ±rdÄ±m" tokenâ€™Ä±nÄ±n sorgusu ($Q$) ile "bankaya" tokenâ€™Ä±nÄ±n anahtarÄ± ($K$) Ã§arpÄ±ldÄ±ÄŸÄ±nda yÃ¼ksek bir skor Ã§Ä±kmasÄ±, modelin bu iki kelimeyi baÄŸlamsal olarak birbiriyle Ã§ok alakalÄ± bulduÄŸunu gÃ¶sterir. SayÄ± sÄ±fÄ±ra yaklaÅŸtÄ±kÃ§a bu alaka zayÄ±flar, negatif deÄŸerler ise bu kelimelerin anlam evreninde birbirini ittiÄŸi ve farklÄ± yÃ¶nlere baktÄ±ÄŸÄ± bir sinyal gibi okunur.</li>
         <li>**Scores matrsinin boyutu** - Embedding boyutuyla deÄŸil, promptâ€™taki token sayÄ±sÄ±yla ilgilidir. Yani 4 token varsa 4Ã—4, 20 token varsa 20Ã—20 bir tablo oluÅŸur.</li>
       </ul>

        <Note>
          <span className="writingNote__icon" aria-hidden="true">!</span> Åu ana kadar tokenâ€™lar arasÄ±ndaki benzerlikleri tamamen serbest bir ÅŸekilde hesapladÄ±k. Yani her token, diÄŸer tÃ¼m tokenâ€™lara bakabiliyor. Buna gelecekteki tokenâ€™lar da dahil. Ama burada ciddi bir problem var: Dil modelleri metni soldan saÄŸa Ã¼retir. Yani model â€œyatÄ±rdÄ±mâ€ kelimesini Ã¼retirken, henÃ¼z yazÄ±lmamÄ±ÅŸ olan kelimeleri bilmemelidir. Aksi halde model, cevabÄ± Ã¶nceden gÃ¶rmÃ¼ÅŸ olur. Bu da Ã¶ÄŸrenmeyi anlamsÄ±z hale getirir.

          Bu noktada **mask** devreye girer.
        </Note>

       **Masking: GeleceÄŸi gÃ¶rmeyi engellemek**

     Skor tablosunu oluÅŸturduk ama burada Ã§ok hayati bir kural devreye giriyor: DÃ¼rÃ¼stlÃ¼k. EÄŸitim sÄ±rasÄ±nda modelin bir sonraki kelimeyi tahmin etmesini istiyoruz. EÄŸer model, skor tablosunda kendisinden sonra gelen kelimelere bakabilirse, bu bir kopya Ã§ekme iÅŸlemi olur.
     
     Bunu engellemek iÃ§in Masking mekanizmasÄ±nÄ± kullanÄ±yoruz:
       <div className="writingArticle__codeRow">
         <Matrix values={[[0.5, -0.1, -0.3, 0.6], [-0.2, 0.1, -0.3, -0.5], [-0.2, -0.3, 0, -0.1], [0.2, -0.2, 0, 0.6]]} title={{ text:"scores" }} prefix={{ text: "mask(" }} suffix={{ text: ")" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">=</div>
         <Matrix values={[[0.5, "-âˆ", "-âˆ", "-âˆ"], [-0.2, 0.1, "-âˆ", "-âˆ"], [-0.2, -0.3, 0, "-âˆ"], [0.2, -0.2, 0, 0.6]]} title={{ text:"masked" }}/>
       </div>

       -inf (eksi sonsuz) deÄŸerleri, modelin "geleceÄŸe" bakmasÄ±nÄ± matematiksel olarak imkansÄ±z hale getirir. Bu sayede her token, sadece kendisinden Ã¶nce gelenlerin baÄŸlamÄ±yla karar vermeye zorlanÄ±r. Yani attention mekanizmasÄ±na bir "zaman disiplini" kazandÄ±rÄ±yoruz.

       **Softmax: SkorlarÄ± aÄŸÄ±rlÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek**

       Maskeleme bittikten sonra elimizde hala ham ve karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± zor sayÄ±lar var. Burada Softmax devreye giriyor ve bu sayÄ±larÄ± alÄ±p toplamÄ± 1 (yani %100) eden, tertemiz olasÄ±lÄ±k aÄŸÄ±rlÄ±klarÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor:
            <div className="writingArticle__codeRow">
         <Matrix values={[[0.5, "-âˆ", "-âˆ", "-âˆ"], [-0.2, 0.1, "-âˆ", "-âˆ"], [-0.2, -0.3, 0, "-âˆ"], [0.2, -0.2, 0, 0.6]]} title={{ text:"masked" }} prefix={{ text: "softmax(" }} suffix={{ text: ")" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">=</div>
         <Matrix values={[[1.0, 0.0, 0.0, 0.0], [0.4, 0.6, 0.0, 0.0], [0.3, 0.3, 0.4, 0.0], [0.3, 0.2, 0.2, 0.4]]} title={{ text:"weights" }}/>
       </div>

       Ortaya Ã§Ä±kan sonuÃ§ artÄ±k bir **Attention AÄŸÄ±rlÄ±k Matrisidir**. Bu matrise baktÄ±ÄŸÄ±nÄ±zda, dÃ¶rdÃ¼ncÃ¼ satÄ±rdaki bir kelimenin, kendisinden Ã¶nceki Ã¼Ã§ kelimeye yÃ¼zde kaÃ§ oranÄ±nda odaklandÄ±ÄŸÄ±nÄ± net bir ÅŸekilde gÃ¶rebilirsiniz. SÄ±fÄ±r olan hÃ¼creler (maskelenmiÅŸ kÄ±sÄ±mlar), modelin o tarafa fiilen hiÃ§ bakmadÄ±ÄŸÄ±nÄ± garantiler.

       **Value (V) ve Output'un oluÅŸmasÄ±: AnlamÄ± paketlemek** 

       Åimdiye kadar sadece "kiminle ne kadar ilgiliyiz?" sorusuna yanÄ±t aradÄ±k. Peki, o ilgili olduÄŸumuz kelimeden hangi bilgiyi alacaÄŸÄ±z? Ä°ÅŸte burada $Value(V)$ matrisi devreye giriyor.

       <div className="writingArticle__codeRow">
         <Matrix values={[[0.2, -0.1, 0.7], [0.9, 0.3, -0.2], [0.4, 0.8, 0.1], [-0.3, 0.2, 0.6]]} title={{ text:"embeddings" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">Ã—</div>
         <Matrix values={[[0.6, -0.2, 0.1], [0.3, 0.7, -0.4], [-0.5, 0.2, 0.8]]} title={{ text:"WV" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">=</div>
         <Matrix values={[[-0.3, 0.0, 0.6], [0.7, 0.0, -0.2], [0.4, 0.5, -0.2], [-0.4, 0.3, 0.4]]} title={{ text:"V" }}/>
       </div>

              V matrisini, embeddingâ€™lerin â€œtaÅŸÄ±nabilir iÃ§erikâ€ hÃ¢li gibi dÃ¼ÅŸÃ¼nebiliriz. Bu aÅŸamada artÄ±k "ne arÄ±yoruz?" sorusu biter; sadece "iÃ§erik" kalÄ±r. Daha Ã¶nce olduÄŸu gibi, burada da eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenilmiÅŸ bir WV matrisi vardÄ±r. Bu matrisi kullanarak token embeddingâ€™lerini V matrisine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rÃ¼z.

       Son adÄ±mda, Softmaxâ€™ten Ã§Ä±kan o hassas aÄŸÄ±rlÄ±klar ile V matrisini Ã§arparÄ±z ($Weights \times V$):
        <div className="writingArticle__codeRow">
         <Matrix values={[[1.0, 0.0, 0.0, 0.0], [0.4, 0.6, 0.0, 0.0], [0.3, 0.3, 0.4, 0.0], [0.3, 0.2, 0.2, 0.4]]} title={{ text:"weights" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">Ã—</div>
         <Matrix values={[[-0.3, 0.0, 0.6], [0.7, 0.0, -0.2], [0.4, 0.5, -0.2], [-0.4, 0.3, 0.4]]} title={{ text:"V" }}/>
         <div className="writingArticle__codeSymbol" aria-hidden="true">=</div>
         <Matrix values={[[-0.3, 0.0, 0.6], [0.3, 0.0, 0.1], [0.3, 0.2, 0.0], [0.0, 0.2, 0.3]]} title={{ text:"output" }}/>
       </div>

       **SonuÃ§: HazÄ±r bir "BaÄŸlamsal VektÃ¶r"**

       <Scatterplot3DDemo 
        id={'output'}
        pointSets={[
          { name: 'Embeddings', points: [[0.2, -0.1, 0.7], [0.9, 0.3, -0.2], [0.4, 0.8, 0.1], [-0.3, 0.2, 0.6]]},
          { name: 'Output', points: [[-0.3, 0, 0.6], [0.3, 0, 0.1], [0.3, 0.2, 0], [0, 0.2, 0.3]]}
         ]}
        labels={["796", "35569", "6922", "5094"]}
       />
      
      Bu Ã§arpÄ±mÄ±n sonucunda elimize geÃ§en Output vektÃ¶rleri, artÄ±k Transformerâ€™a giren o ilk "statik" kelimeler deÄŸildir. Bunlar:

      <ul className="writingArticle__TokenizerList">  
         <li>CÃ¼mlenin parÃ§asÄ± olarak anlam kazanmÄ±ÅŸ,</li>
          <li>BaÄŸlamÄ±n iÃ§ine yerleÅŸtirilmiÅŸ,</li>
         <li>Hangi kelimenin ne kadar Ã¶nemli olduÄŸu bilgisiyle yoÄŸurulmuÅŸ **dinamik temsillerdir**.</li>
       </ul>
     
     Ancak heyecanlanmayÄ±n, model hala bir kelime Ã¼retmedi! Bu Ã§Ä±kan vektÃ¶rler nihai karar deÄŸil; sadece bir sonraki aÅŸamada **(Feedforward (FFN))** daha da iÅŸlenmek Ã¼zere hazÄ±rlanmÄ±ÅŸ, anlamÄ± derinleÅŸmiÅŸ ara temsilcilerdir.
             
                <h3 className="writingArticle__transformerTitle">Feedforward: AnlamÄ±n piÅŸirildiÄŸi mutfak</h3>

      Attention mekanizmasÄ± gÃ¶revini tamamlayÄ±p her token iÃ§in diÄŸer kelimelerle harmanlanmÄ±ÅŸ o baÄŸlamsal matrisi (Output) Ã¶nÃ¼mÃ¼ze koyduÄŸunda, aslÄ±nda sadece "kimin kiminle ne kadar iliÅŸkili olduÄŸunu" Ã§Ã¶zmÃ¼ÅŸ oluruz. Ancak sadece konuÅŸmak yetmez; bu konuÅŸmadan Ã§Ä±kan mesajÄ±n iÅŸlenmesi, rafine edilmesi ve kalÄ±cÄ± bir "bilgiye" dÃ¶nÃ¼ÅŸmesi gerekir.  
      
      **Feedforward (FFN)** katmanÄ± tam olarak burasÄ±dÄ±r. Attention'dan Ã§Ä±kan o baÄŸlamsal vektÃ¶rleri alÄ±r ve onlarÄ± adeta bir mutfaÄŸa sokar:
        <ul className="writingArticle__TokenizerList">
         <li>**Bireysel iÅŸleme** - Attention tÃ¼m tokenâ€™lara bir bÃ¼tÃ¼n olarak, aralarÄ±ndaki iliÅŸkiyi tartarak bakarken; Feedforward her bir tokenâ€™Ä± (vektÃ¶rÃ¼) kendi Ã¶zelinde ele alÄ±r. "Senin diÄŸer kelimelerle iliÅŸkin bu, ÅŸimdi bu yeni kimliÄŸini daha derin bir mantÄ±k sÃ¼zgecinden geÃ§irelim" der.</li>
         <li>**DerinleÅŸtirme ve dÃ¼nya bilgisi** - BirÃ§ok araÅŸtÄ±rmacÄ±, modellerin eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrendiÄŸi "TÃ¼rkiyeâ€™nin baÅŸkenti Ankaraâ€™dÄ±r" gibi temel dÃ¼nya bilgilerinin bÃ¼yÃ¼k oranda bu Feedforward katmanlarÄ±ndaki aÄŸÄ±rlÄ±klarda saklandÄ±ÄŸÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼yor. BurasÄ±, modelin basit bir istatistiksel eÅŸleÅŸmeden Ã§Ä±kÄ±p, kavramsal bir derinlik kazandÄ±ÄŸÄ± yerdir.</li>
         <li>**DoÄŸrusal olmayan (Non-linear) dokunuÅŸ** - Bu katmanda veriler karmaÅŸÄ±k matematiksel fonksiyonlardan (genellikle ReLU veya GELU) geÃ§er. Bu sayede model, dilin iÃ§indeki o dÃ¼z olmayan, karmaÅŸÄ±k ve Ã§ok katmanlÄ± mantÄ±k yapÄ±larÄ±nÄ± taklit etme yeteneÄŸi kazanÄ±r.</li>
       </ul>

       Ã–zetle; **Attention** "iliÅŸkiyi" kurar, **Feedforward** o iliÅŸkiden Ã§Ä±kan "anlamÄ±" inÅŸa eder.

        <Note>
          <span className="writingNote__icon" aria-hidden="true">!</span>
          Teknik detaylarÄ± bir kenara bÄ±rakÄ±rsak: Feedforward katmanÄ±nÄ±n yaptÄ±ÄŸÄ±
          ÅŸey, Attention ile toplanan bilgiden gerÃ§ekten iÅŸe yarayan
          kombinasyonlarÄ± Ã§Ä±karmak.
      
          Unutmamak gerekir ki; Feedforward katmanÄ± yeni bir baÄŸlam eklemez.
          Dikkati yeniden daÄŸÄ±tmaz veya tokenâ€™lar arasÄ±nda yeni iliÅŸkiler
          kurmaz. Tek gÃ¶revi, Attention Ã§Ä±ktÄ±sÄ±nÄ± daha anlamlÄ± ve daha
          gÃ¼Ã§lÃ¼ bir iÃ§ temsile dÃ¶nÃ¼ÅŸtÃ¼rmek.

          Ã–zetle: Attention, hangi bilginin Ã¶nemli olduÄŸunu
          seÃ§er; Feedforward, seÃ§ilen bu bilgiyi yoÄŸurur ve derinleÅŸtirir. Bu
          yÃ¼zden Transformer mimarisinde bu iki adÄ±m her zaman ayrÄ±lmaz bir ikili
          olarak Ã§alÄ±ÅŸÄ±r; biri baÄŸÄ± kurar, diÄŸeri o baÄŸÄ± iÅŸler.
        </Note>

      <h3 className="writingArticle__transformerTitle">Transformer BloklarÄ± (Attention + Feedforward): DÃ¶ngÃ¼nÃ¼n defalarca tekrarlanmasÄ±</h3>

      Bu noktada durup bir perspektif kazanalÄ±m: Bir Transformer bloÄŸu (Attention + Feedforward) bittiÄŸinde sÃ¼reÃ§ hemen sona ermiyor. <a className="writingArticle__link" href="https://poloclub.github.io/transformer-explainer/" target="_blank" rel="noreferrer">PaylaÅŸtÄ±ÄŸÄ±mÄ±z o interaktif gÃ¶rsellerdeki</a> "11 more identical Transformer Blocks" ifadesi, bu sÃ¼recin dikey bir maraton olduÄŸunu fÄ±sÄ±ldÄ±yor.

    Her bir bloktan geÃ§en veri, bir Ã¼stteki blokta tekrar Attention ve Feedforward sÃ¼zgeÃ§lerinden geÃ§iyor. Bu dikey yolculuk sÄ±rasÄ±nda anlam her katmanda biraz daha rafine ediliyor, soyutlaÅŸÄ±yor ve derinleÅŸiyor. Bu iÅŸlem binlerce kez tekrarlandÄ±ÄŸÄ±nda, karÅŸÄ±mÄ±za o her ÅŸeyi bilen, mantÄ±k yÃ¼rÃ¼tebilen devasa zeka Ã§Ä±kÄ±yor. En alt katmanlarda sadece kelime iliÅŸkilerini Ã§Ã¶zen model, en Ã¼st katmanlara Ã§Ä±ktÄ±ÄŸÄ±nda artÄ±k cÃ¼mlenin tonunu, niyetini ve mantÄ±ksal kurgusunu tam olarak kavramÄ±ÅŸ oluyor.

        <h2>Transformer'dan Ã§Ä±kÄ±ÅŸ: OlasÄ±lÄ±klarÄ±n sahnesi </h2>

    TÃ¼m katmanlardaki "maraton" bittikten sonra, elimizdeki vektÃ¶rler artÄ±k en olgun haline ulaÅŸtÄ±. Ama hala birer sayÄ± dizisiler. Ä°ÅŸte bu noktada Transformerâ€™dan Ã§Ä±kÄ±p Output (Probabilities) katmanÄ±na, yani modelin bize bir cevap vereceÄŸi son aÅŸamaya geÃ§iyoruz:

    <ol className="writingArticle__TokenizerList writingArticle__TokenizerList--ordered">
         <li>**SÃ¶zlÃ¼kle VedalaÅŸma (Linear Layer)** - Model, bu olgunlaÅŸmÄ±ÅŸ vektÃ¶rÃ¼ alÄ±r ve onu sahip olduÄŸu on binlerce kelimelik devasa hazinesine yansÄ±tÄ±r. Her bir kelime iÃ§in "Bir sonraki parÃ§a ben olabilirim!" diyen bir adaylÄ±k puanÄ± hesaplanÄ±r.</li>

         Ã–rneÄŸin; "ParayÄ± bankaya..." diye baÅŸlayan bir cÃ¼mlede, bir sonraki token iÃ§in [yatÄ±rdÄ±m], [gÃ¶nderdim], [bÄ±raktÄ±m] gibi adaylar yÃ¼ksek puanlar alÄ±rken, [elma] gibi alakasÄ±z adaylar listenin en sonunda kalÄ±r.

         <li>**OlasÄ±lÄ±k Filtresi (Softmax)** - Bu puanlar Softmax sÃ¼zgecinden geÃ§erek, toplamÄ± %100 eden net yÃ¼zdelere dÃ¶nÃ¼ÅŸÃ¼r:</li>
             <ul className="writingArticle__TokenizerList">
         <li>[yatÄ±rdÄ±m] -> %88</li>
         <li>[gÃ¶nderdim] -> %7</li>
         <li>[havale ettim] -> %4</li>
          <li>...</li>
       </ul>
         <li>**Kelimelerin DoÄŸuÅŸu (Sampling & Temperature)** - Model her zaman en yÃ¼ksek olasÄ±lÄ±klÄ± kelimeyi seÃ§mez. Sampling (Ã¶rnekleme) stratejileri tam burada devreye girer. Temperature (SÄ±caklÄ±k) ayarÄ±yla modelin ne kadar "yaratÄ±cÄ±" veya "risk alan" bir seÃ§im yapacaÄŸÄ±nÄ± belirlersiniz. Top-k ve Top-p gibi tekniklerle de olasÄ±lÄ±ÄŸÄ± Ã§ok dÃ¼ÅŸÃ¼k olan "mantÄ±ksÄ±z" kelimeler elenir ve modelin tutarlÄ± ama doÄŸal bir cevap vermesi saÄŸlanÄ±r.</li>
         </ol>

          <h2>Ve dÃ¶ngÃ¼ tamamlanÄ±r: Otoregresif sÃ¼reÃ§</h2>

          Buradaki asÄ±l "zeka" illÃ¼zyonu ÅŸurada gizli: Model tek seferde koca bir paragraf Ã¼retmiyor. Sadece tek bir kelime (token) seÃ§iyor. Ã–rneÄŸin [yatÄ±rdÄ±m] kelimesini seÃ§tiyse, ÅŸimdi bu kelimeyi alÄ±yor ve kendi girdi (input) kÄ±smÄ±nÄ±n en sonuna ekliyor: "ParayÄ± bankaya yatÄ±rdÄ±m..."
          
          Åimdi tÃ¼m bu devasa iÅŸlem bandÄ± (Tokenizer -> Embedding -> Attention -> FFN), cÃ¼mleye devam edecek bir sonraki kelimeyi (belki bir nokta, belki "ve" baÄŸlacÄ±) bulmak iÃ§in saniyeler iÃ§inde tekrar koÅŸturuyor. Siz akÄ±cÄ± bir cevap okurken arka planda bu "tahmin motoru" her bir parÃ§a iÃ§in en baÅŸtan ter dÃ¶kÃ¼yor.

       <div className="writingArticle__callout">
       
       **YazarÄ±n notu: Neden buradayÄ±z?** 
       
       BÃ¼yÃ¼k Dil Modelleri (LLM) dÃ¼nyasÄ±nda her gÃ¼n yeni bir model, yeni bir benchmark ve yeni bir heyecan dalgasÄ±yla karÅŸÄ±laÅŸÄ±yoruz. Ancak bu devasa makinenin altÄ±ndaki Ã§arklarÄ±n nasÄ±l dÃ¶ndÃ¼ÄŸÃ¼nÃ¼ anlamak; sadece teknolojiyi kullanmakla, ona yÃ¶n vermek arasÄ±ndaki o ince Ã§izgiyi belirliyor.
       
       Bu yazÄ±da, bir kelimenin sayÄ±ya dÃ¶nÃ¼ÅŸmesinden, milyarlarca matrisin birbiriyle 'konuÅŸarak' anlam yaratmasÄ±na kadar olan o bÃ¼yÃ¼leyici yolculuÄŸu birlikte izledik. AmacÄ±m, bu karmaÅŸÄ±k gÃ¶rÃ¼nen dÃ¼nyayÄ± biraz daha ÅŸeffaf ve anlaÅŸÄ±lÄ±r kÄ±larak, hepimizi bekleyen o bÃ¼yÃ¼k geleceÄŸe bir tÄ±k daha hazÄ±rlamaktÄ±.
       
       Peki, bu temel mimariyi neden bu kadar detaylÄ± konuÅŸtuk? Ã‡Ã¼nkÃ¼ bir sonraki duraklarda, bu 'kelime tahmin motorlarÄ±nÄ±n' sadece konuÅŸan botlar olmaktan Ã§Ä±kÄ±p, bizim yerimize kararlar alan ve aksiyon geÃ§en Agentic AI sistemlerine nasÄ±l dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ konuÅŸacaÄŸÄ±z. Ã‡Ã¼nkÃ¼ gelecek, sadece tahmin edenlerin deÄŸil, o tahminleri hedeflere dÃ¶nÃ¼ÅŸtÃ¼ren ajanlarÄ±n olacak.
       
       Bir sonraki yazÄ±da, bu mimarinin Ã¼zerine inÅŸa edilen o akÄ±llÄ± dÃ¼nyada buluÅŸmak Ã¼zere!
                <br />

       **P.S.** Bu yazÄ±nÄ±n 'debug' sÃ¼recinde yanÄ±mda olan, teknik kÄ±sÄ±mlardaki pÃ¼rÃ¼zleri birlikte giderdiÄŸimiz developer arkadaÅŸÄ±m <a className="writingArticle__link" href="https://www.linkedin.com/in/arindenizkoklu/" target="_blank" rel="noreferrer">ArÄ±n Deniz KÃ¶klÃ¼</a>â€™ye destekleri iÃ§in teÅŸekkÃ¼rler!

       </div>
---
